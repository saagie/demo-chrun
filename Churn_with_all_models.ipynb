{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibis\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass \n",
    "  \n",
    "try: \n",
    "    p = getpass.getpass() \n",
    "except Exception as error: \n",
    "    print('ERROR', error) \n",
    "else: \n",
    "    print('Password entered:') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data from Impala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = ibis.hdfs_connect(host=os.environ['HDFS_HOST'], port=50070)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_impala = ibis.impala.connect(host=os.environ['IP_IMPALA'], port=21050, \\\n",
    "  hdfs_client=hdfs, user=os.environ['CHANDIMA_LOGIN'], password=getpass.getpass(), \\\n",
    "  auth_mechanism='PLAIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "requete =client_impala.sql('SELECT * FROM open_data.clean_bank')\n",
    "df = requete.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check co-relation between the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify the Distribution of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = df.copy()\n",
    "copy.dropna(inplace = True)\n",
    "sns.distplot(copy[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = df.copy()\n",
    "copy.dropna(inplace = True)\n",
    "sns.distplot(copy[\"balance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = df.copy()\n",
    "copy.dropna(inplace = True)\n",
    "sns.distplot(copy[\"estimatedsalary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = df.copy()\n",
    "copy.dropna(inplace = True)\n",
    "sns.distplot(copy[\"tenure\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = df.copy()\n",
    "copy.dropna(inplace = True)\n",
    "sns.distplot(copy[\"isactivemember\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age and Balance are normaly distributed. Tenure and isactivemember are uniformly disributed. These distibutions are useful when to replace null values.\n",
    "Either by mean or median. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn according to Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('gender',data=df,hue='exited').set_title('Churn with Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn according to Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('geography',data=df,hue='exited').set_title('Churn According to Geography')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn acording to Age Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cat = df[['age','exited']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [0, 15, 25, 35, 45, 55,120]\n",
    "g = ['Children 0 - 15','Teenagers 16-25','Youth 26-35','Adult 36- 45','Mature 46- 55','Old 56+']\n",
    "age_cat['Age_Category'] = pd.cut(age_cat['age'], bins=r, labels=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = age_cat.groupby('Age_Category').mean()\n",
    "df_3.reset_index(inplace=True)\n",
    "plt.figure(figsize=(6,8))\n",
    "sns.barplot(x='Age_Category', y='exited', data=df_3,palette='plasma').set_title('Churn vs Age Category')\n",
    "plt.xticks(plt.xticks()[0],g , rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(df['numofproducts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding our categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cat = ['geography', 'gender']\n",
    "training_data = pd.get_dummies(df, columns = list_cat, prefix = list_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to reshape our data since the Scaler takes in arrays\n",
    "creditscore_train = np.array(training_data[\"creditscore\"]).reshape(-1, 1)\n",
    "age_train = np.array(training_data[\"age\"]).reshape(-1, 1)\n",
    "balance_train = np.array(training_data[\"balance\"]).reshape(-1, 1)\n",
    "estimatedsalary_train = np.array(training_data[\"estimatedsalary\"]).reshape(-1, 1)\n",
    "\n",
    "training_data[\"creditscore\"] = scaler.fit_transform(creditscore_train)\n",
    "training_data[\"age\"] = scaler.fit_transform(age_train)\n",
    "training_data[\"balance\"] = scaler.fit_transform(balance_train)\n",
    "training_data[\"estimatedsalary\"] = scaler.fit_transform(estimatedsalary_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(training_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['exited','rownumber','customerid','surname'], axis=1).values\n",
    "y_train = train[\"exited\"].values\n",
    "X_test = test.drop(['exited','rownumber','customerid','surname'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import sklearn algorithms and libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use different classification algorithms. Use libraries to mesure the model performance and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validation Data Set: to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf_parameters = {\"n_estimators\": [2, 4, 8, 10, 15, 20, 25, 30], \"criterion\": [\"gini\", \"entropy\"], \n",
    "                 \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n",
    "                 \"max_depth\": [2, 3, 5, 10], \"min_samples_split\": [2, 3, 5, 7, 10]}\n",
    "\n",
    "grid_rf = GridSearchCV(rf, rf_parameters, scoring=make_scorer(accuracy_score))\n",
    "grid_rf.fit(X_training, y_training)\n",
    "\n",
    "rf = grid_rf.best_estimator_\n",
    "\n",
    "rf.fit(X_training, y_training)\n",
    "pred_rf = rf.predict(X_valid)\n",
    "acc_rf = accuracy_score(y_valid, pred_rf)\n",
    "\n",
    "print(\"The Score of Random Forest is: \" + str(acc_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_rf = precision_score(y_valid,pred_rf, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Precision of Random Forest is: \" + str(precision_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_rf = recall_score(y_valid,pred_rf, average='weighted')\n",
    "print(\"The Recall of Random Forest is: \" + str(recall_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_rf = f1_score(y_valid,pred_rf, average='weighted')\n",
    "print(\"The F1 score of Random Forest is: \" + str(F1_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logostic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg_parameters = {\"penalty\": [\"l2\", \"elasticnet\"], \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
    "                     \"max_iter\": [4000], ,\n",
    "                     \"multi_class\" :[\"ovr\"]\n",
    "                    }\n",
    "\n",
    "grid_logreg = GridSearchCV(logreg, logreg_parameters, scoring=make_scorer(accuracy_score))\n",
    "grid_logreg.fit(X_training, y_training)\n",
    "\n",
    "logreg = grid_logreg.best_estimator_\n",
    "\n",
    "logreg.fit(X_training, y_training)\n",
    "pred_logreg = logreg.predict(X_valid)\n",
    "acc_logreg = accuracy_score(y_valid, pred_logreg)\n",
    "\n",
    "print(\"The Score of Logistic Regression is: \" + str(acc_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_lg = precision_score(y_valid,pred_logreg, average='weighted')\n",
    "print(\"The precision of Logistic Regression is: \" + str(precision_lg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_lg = recall_score(y_valid,pred_logreg, average='weighted')\n",
    "print(\"The recall of Logistic Regression is: \" + str(recall_lg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_lg = recall_score(y_valid,pred_logreg, average='weighted')\n",
    "print(\"The F1 score of Logistic Regression is: \" + str(F1_lg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn_parameters = {\"n_neighbors\": [5,6,7,8,9,10],\"leaf_size\": [2,4,6,10,15,20, 30, 50],\n",
    "                  \"weights\": [\"uniform\", \"distance\"], \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\",\"brute\"]\n",
    "                  }\n",
    "\n",
    "grid_knn = GridSearchCV(knn, knn_parameters, scoring=make_scorer(accuracy_score))\n",
    "grid_knn.fit(X_training, y_training)\n",
    "\n",
    "knn = grid_knn.best_estimator_\n",
    "\n",
    "knn.fit(X_training, y_training)\n",
    "pred_knn = knn.predict(X_valid)\n",
    "acc_knn = accuracy_score(y_valid, pred_knn)\n",
    "\n",
    "print(\"The Score of KNeighbors is: \" + str(acc_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_knn = precision_score(y_valid,pred_knn, average='weighted')\n",
    "print(\"The precision of KNN is: \" + str(precision_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_knn = recall_score(y_valid,pred_knn, average='weighted')\n",
    "print(\"The recall of KNN is: \" + str(recall_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_knn = recall_score(y_valid,pred_knn, average='weighted')\n",
    "print(\"The FI score of KNN is: \" + str(F1_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "\n",
    "gnb_parameters = {}\n",
    "\n",
    "grid_gnb = GridSearchCV(gnb, gnb_parameters, scoring=make_scorer(accuracy_score))\n",
    "grid_gnb.fit(X_training, y_training)\n",
    "\n",
    "gnb = grid_gnb.best_estimator_\n",
    "\n",
    "gnb.fit(X_training, y_training)\n",
    "pred_gnb = gnb.predict(X_valid)\n",
    "acc_gnb = accuracy_score(y_valid, pred_gnb)\n",
    "\n",
    "print(\"The Score of Gaussian NB is: \" + str(acc_gnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_gnb = precision_score(y_valid,pred_gnb, average='weighted')\n",
    "print(\"The precision of Gaussian NB is: \" + str(precision_gnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_gnb = recall_score(y_valid,pred_gnb, average='weighted')\n",
    "print(\"The recall of Gaussian NB is: \" + str(recall_gnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_gnb = recall_score(y_valid,pred_gnb, average='weighted')\n",
    "print(\"The F1 score of Gaussian NB is: \" + str(F1_gnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt_parameters = {\"max_features\": [\"auto\", \"sqrt\", \"log2\"],\"min_samples_split\": [2,3,5,9,12,13,14,15,20],\n",
    "                  \"min_samples_leaf\":[2,5,6,9,10],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"], \n",
    "                  \"splitter\": [\"best\", \"random\"], \n",
    "                  }\n",
    "\n",
    "grid_dt = GridSearchCV(dt, dt_parameters, scoring=make_scorer(accuracy_score))\n",
    "grid_dt.fit(X_training, y_training)\n",
    "\n",
    "dt = grid_dt.best_estimator_\n",
    "\n",
    "dt.fit(X_training, y_training)\n",
    "pred_dt = dt.predict(X_valid)\n",
    "acc_dt = accuracy_score(y_valid, pred_dt)\n",
    "\n",
    "print(\"The Score of Decision Tree is: \" + str(acc_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_dt = precision_score(y_valid,pred_dt, average='weighted')\n",
    "print(\"The precision of DT is: \" + str(precision_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_dt = recall_score(y_valid,pred_dt, average='weighted')\n",
    "print(\"The recall of DT is: \" + str(recall_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_dt = recall_score(y_valid,pred_dt, average='weighted')\n",
    "print(\"The F1 score of DT is: \" + str(F1_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linsvc = LinearSVC()\n",
    "\n",
    "linsvc_parameters = {\"penalty\" :[\"l2\"], \"multi_class\": [\"ovr\", \"crammer_singer\"], \n",
    "                     \"fit_intercept\": [True, False],\n",
    "                     \"C\": [ 1,10], \n",
    "                     \"max_iter\": [100000]}\n",
    "\n",
    "grid_linsvc = GridSearchCV(linsvc, linsvc_parameters, scoring=make_scorer(accuracy_score))\n",
    "grid_linsvc.fit(X_training, y_training)\n",
    "\n",
    "linsvc = grid_linsvc.best_estimator_\n",
    "\n",
    "linsvc.fit(X_training, y_training)\n",
    "pred_linsvc = linsvc.predict(X_valid)\n",
    "acc_linsvc = accuracy_score(y_valid, pred_linsvc)\n",
    "\n",
    "print(\"The Score of LinearSVC is: \" + str(acc_linsvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_svc = precision_score(y_valid,pred_linsvc, average='weighted')\n",
    "print(\"The precision of Linear SVC is: \" + str(precision_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_svc = recall_score(y_valid,pred_linsvc, average='weighted')\n",
    "print(\"The recall of Linear SVC is: \" + str(recall_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_svc = recall_score(y_valid,pred_linsvc, average='weighted')\n",
    "print(\"The F1 score of Linear SVC is: \" + str(F1_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xg_clf = XGBClassifier()\n",
    "\n",
    "parameters_xg = {\"objective\" : [\"reg:linear\"], \"n_estimators\" : [5, 10, 15, 20]}\n",
    "\n",
    "grid_xg = GridSearchCV(xg_clf, parameters_xg, scoring=make_scorer(accuracy_score))\n",
    "grid_xg.fit(X_training, y_training)\n",
    "\n",
    "xg_clf = grid_xg.best_estimator_\n",
    "\n",
    "xg_clf.fit(X_training, y_training)\n",
    "pred_xg = xg_clf.predict(X_valid)\n",
    "acc_xg = accuracy_score(y_valid, pred_xg)\n",
    "\n",
    "print(\"The Score for XGBoost is: \" + str(acc_xg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_xg = precision_score(y_valid,pred_xg, average='weighted')\n",
    "print(\"The precision of xgboost is: \" + str(precision_xg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_xg = recall_score(y_valid,pred_xg, average='weighted')\n",
    "print(\"The recall of xgboost is: \" + str(recall_xg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_xg = recall_score(y_valid,pred_xg, average='weighted')\n",
    "print(\"The F1 score of xgboost is: \" + str(F1_xg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance = pd.DataFrame({\n",
    "   \n",
    "    \"Accuracy_Score\": [acc_rf,acc_logreg,acc_knn,acc_gnb,acc_dt,acc_linsvc,acc_xg],\n",
    "    \"Precision\": [precision_rf, precision_lg, precision_knn, precision_gnb, precision_dt,precision_svc, precision_xg],\n",
    "    \"Recall\" : [recall_rf, recall_lg, recall_knn, recall_gnb, recall_dt, recall_svc, recall_xg],\n",
    "    \"F1 Score\": [F1_rf, F1_lg, F1_knn, F1_gnb, F1_dt, F1_svc, F1_xg],\n",
    "     \"Model\": [\"Random Forest Classifier\", \"Logistic Regression\", \"KNN\", \n",
    "              \"GaussianNB\", \"DecisionTree Classifier\",\"Linear SVC\", \"XGBoost\"]\n",
    "})\n",
    "\n",
    "\n",
    "model_performance = model_performance[['Model', 'Accuracy_Score', 'Precision', 'Recall', 'F1 Score']]\n",
    "model_performance.sort_values(by=\"Precision\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained the models and measured their accuracy, precision, Recall and F1 Score. According to received results, the best model is\n",
    "XGBoost due to having high Precision, Recall and F1 score. We can use this model to predict our unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sumbission = xg_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test[\"exited\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2 = pd.DataFrame({\n",
    "        \"Id\": test[\"customerid\"],\n",
    "        \"Customer_name\":test[\"surname\"],\n",
    "        \"expected_Exited\": y_test,\n",
    "        \"predicted_Exited\": result_sumbission.round()\n",
    "    })\n",
    "\n",
    "submission2 = submission2[['Id','Customer_name','expected_Exited','predicted_Exited']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_valid, pred_xg)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('Actual labels'); \n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['0', '1'])\n",
    "ax.yaxis.set_ticklabels(['1', '0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save churn_bank_customers.csv file in to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "client_hdfs = InsecureClient('http://192.168.56.10:50070', user=os.environ['CHANDIMA_LOGIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Dataframe to hdfs\n",
    "with client_hdfs.write('/user/chandima.pondapelage/Bank/result_csv/churn_bank_customers.csv',encoding = 'utf-8') as writer:\n",
    "    submission2.to_csv(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
